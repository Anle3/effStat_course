---
title: "Machine Learning in clinical development \n "
subtitle: "The what?, the why?, and (a bit of) the how?"
date: "`r Sys.Date()`"
author: Eliana Garcia Cossio, Antigoni Elefsinioti, Sandra Gonzalez Maldonado
output:
  rmdformats::downcute:
    self_contained: true
    thumbnails: false
    lightbox: false
    gallery: false
    highlight: tango
    number_sections: false
    theme: spacelab
    code_folding: hide
---


**Demo: Random Survival Forests on simulated data**


This demo aims at providing an overview of applying Random Survival Forests on simulated data.
We are splitting the demo in the following steps

1. Data generation
2. Data training-testing split
3. Data pre-processing
4. Setup model
5. Hyperparameter tuning
6. Final model training and performance evaluation
7. Explainable AI
7.1 VIMP (Variable Importance)
7.2 Partial dependency plots (PDPs) 



```{r knitsetup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, out.width = "100%")


ggplot2::theme_set(theme_light() +
                     theme(
                       strip.background = element_rect(fill = "#00617F"),
                       strip.text = element_text(colour = "#FFFFFF")
                     )
)
```

# Data generation

We simulate data and also right-censored survival outcome, using a proportional hazard model with time-constant baseline hazard, such as 

$Surv(time,status) \sim 0.3*bm01+0.6*bm02+0.4*bm03+0.1*bm04-trt-1.5*trt*bm01$
$-1.5*trt*bm05-1.5*trt*bm06 + noise(bm07...bm20)$

Where:

* Main effects
$0.3*bm01+0.6*bm02+0.4*bm03+0.1*bm04-trt$
* Interaction with treatment
$-1.5*trt*bm01-1.5*trt*bm05-1.5*trt*bm06$

The dataset is small on purpose, so the knitting runtime is short. Still some chunks might take a bit longer to run . We set these chunks to eval=FALSE and saved their outcomes  in .qs files. Those files are being read in the next chunk. If one wants to run these chunks please set eval to TRUE

This function `biospear::simdata` simulates a data set with survival outcome with given active biomarkers (prognostic and/or interacting with the treatment).

```{r, fig.width= 8, fig.height= 4}
library(biospear)
library(tidyverse)
library(GGally)


#The simdata function generates p Gaussian unit-variance (σ = 1) biomarkers including autoregressive correlation (σ_ij = b.corr^|i-j|) within b.corr.by-biomarker blocks. 
#The number of active biomarkers and their effect sizes (in log-scale) can be specified using q.main and beta.main for the true prognostic biomarkers and using q.inter 
#and beta.inter for the true treatment-effect modifiers. A total of n patients is generated and randomly assigned to the experimental (coded as +0.5, with probability prob.tt) 
#and control treatment (coded as -0.5). The treatment effect is specified using alpha.tt (in log-scale). Survival times are generated using a Weibull with shape wei.shape 
#(i.e. 1 = exponential distribution) and patient-specific scale depending on the baseline median survival time m0 and the biomarkers values of the patient. 
#Censor status is generated by considering independant censoring from a U(fu, fu + recr) distribution, reflecting a trial with recr years of accrual and fu years of follow-up. 
#Another data set with the same characteristics as the one generated by simdata can be obtained by using the simdataV function.

n = 2000#200
p = 20

# Set a seed for reproducibility
set.seed(567)

Xprog <- c("bm01","bm02","bm03","bm04")
Xpred <- c("bm01","bm05","bm06")
Xprogpred <- dplyr::intersect(Xpred,Xprog)
XsolelyProg <- dplyr::setdiff(Xprog, Xpred)    # set differentiation 
q = length(Xpred)

dataBM <- biospear::simdata(
  # the sample size
  n = n,
  # the number of biomarkers                          
  p = p,     
  # the number of true prognostic biomarkers.
  q.main = 4 ,    
  # the number of true biomarkers interacting with the treatement.
  q.inter = 3, 
  # the treatment assignment probability.
  prob.tt = 0.5,         
  # baseline median survival time. 
  m0 = 5,            
  # the effect of the treatment (in log-scale)~ 0.37
  alpha.tt = -1,         
  # the effect of the prognostic biomarkers (in log-scale)
  beta.main = c(0.3, 0.6, 0.4, 0.1),    
  # the effect of the biomarkers interacting with the treatment (in log-scale)
  beta.inter = rep(-1.5, 3),    
  # the correlation between biomarker blocks
  b.corr = 0.8, 
  # the size of the blocks of correlated biomarkers
  b.corr.by = 2,    
  # the shape parameter of the Weibull distribution
  wei.shape = 1,  
  # the recruitment period duration
  recr = 1,                             
  # the follow-up period duration                          
  fu = 10,                              
  # the scale multiplicative factor for times (i.e. 1 = times in years)                          
  timefactor = 1,                       
  # the list of the prognostic biomarkers (not mandatory).                          
  active.main = Xprog,    
  # the list of the biomarkers interacting with the treatment (not mandatory).
  active.inter = Xpred                  
)

dataBM <- dataBM %>% 
  rename(.status = status,  # renaming status, time 
         .time = time) %>% 
  # before: A total of n patients is generated and randomly assigned to the experimental 
  #(coded as +0.5, with probability prob.tt) and control treatment (coded as -0.5
  dplyr::mutate(.trt = ifelse(treat == "-0.5",          
                              "control", 
                              "active"))  %>% # preparing the data in suited form. -0.5 was 
  select(-treat)



biomarker_names <- colnames(dataBM)[2:21]
biomarker_names

# Visualizing the correlation structure
ggcorr(dataBM %>% 
         select(-.trt, -.status, -.time), 
       layout.exp = 1, 
       label = TRUE, 
       label_size = 2, 
       hjust = 0.75, 
       size = 3, 
       color = "grey50")


# Introducing missing values in the data set

# Set a seed for reproducibility
set.seed(123)

# Create a copy of the dataset
dataBM_with_na <- dataBM

# Define the proportion of missing values to introduce
proportion_missing <- 0.005  # 0.5% missing values

# Calculate the number of missing values to introduce
n_missing <- round(proportion_missing * nrow(dataBM_with_na) * ncol(dataBM_with_na))

# Randomly select indices for missing values
missing_indices <- sample(1:(nrow(dataBM_with_na) * ncol(dataBM_with_na)), n_missing)

# Introduce NA values systematically
for (index in missing_indices) {
  row <- (index - 1) %/% ncol(dataBM_with_na) + 1
  col <- (index - 1) %% ncol(dataBM_with_na) + 1
  dataBM_with_na[row, col] <- NA
}

# for our demo we will assume that we won't do any imputation on the outcomes and treatment information
# therefore, we need to remove missing values on these variables

dataBM_with_na <- dataBM_with_na %>% 
  filter(!is.na(.trt), !is.na(.time), !is.na(.status))

# View the first few rows of the modified dataset
#head(dataBM_with_na)


# creating also other categorical variables bm07 and bm09

# data <- dataBM_with_na %>% 
#   dplyr::mutate(bm07 = ifelse(bm07 < -1,          # # below -1 y otherwise N
#                               "Y", 
#                               "N"),
#                 bm09 = ifelse(bm09 > 1,          # # above 1 y otherwise N
#                               "Y", 
#                               "N")) %>% 
#   mutate(bm07 = as.factor(bm07),
#          bm09 = as.factor(bm09),
#          .trt = as.factor(.trt))

data <- dataBM_with_na %>% 
  mutate(.trt = as.factor(.trt))
# adding .id column
data <- tibble::rownames_to_column(data, ".id") 

data <- data%>% 
  mutate(.id = paste0("id_", .id))


```


```{r import_packages}
#load utility packages
library(kableExtra)
library(here)
library(tictoc)
library(stringr)
library(qs)
library(skimr)

```


## Exploring the data

```{r raw_data_explore, skimr_digits = 2}


data%>% skimr::skim()

```

# Data training-testing split


We can start by loading the tidymodels metapackage and splitting our data into training and testing sets 

```{r split_data}
library(tidymodels)

set.seed(123)
#create a single binary split of the data into a training set and testing set, 
#doing a stratified approach based on the outcome .status

data_split <- rsample::initial_split(data, strata = .status)

#extract  the resulting data 
data_train <- rsample::training(data_split)
data_test <- rsample::testing(data_split)


```

# Data pre-processing

We pre-process the training data and apply exactly the same step to the test data.

- Imputing the missing data using k-nearest neighbors (with the exception of .trt)
- NA-omit of predictors if after imputation we still have missing values
- Near zero variance features
- Correlation filter with r= 0.8

Other steps include: Normalization of data (given our data generation process we do 
not require this step) or dummy encoding (for Random Forest this is not necessary).

Below we can see how the data has been transformed.


```{r data_preprocess, results ='asis', message = TRUE, prompt=TRUE}

#textrecipes contain extra steps for the recipes package for preprocessing text data.
library(textrecipes)

# make a recipe using the training data ####
tte_recipe <-
  recipes::recipe(formula = .time + .status ~ ., data = data_train) %>%
  recipes::update_role(.id, new_role = "id") %>%
  recipes::update_role(c(.time, .status), new_role = "outcome") %>%
  recipes::step_impute_knn(recipes::all_predictors(), -.trt) %>%
  recipes::step_naomit(recipes::all_predictors()) %>%
  recipes::step_nzv(recipes::all_predictors(),
                    freq_cut = 95 / 5,
                    unique_cut = 10) #%>%
  #recipes::step_normalize(recipes::all_numeric_predictors()) %>%
 # recipes::step_corr(recipes::all_numeric_predictors(), threshold = 0.75)#%>%

#print(tte_recipe)

#prepare new data####
prep_tte_recipe <- tte_recipe %>%
  recipes::prep()

print(prep_tte_recipe)

prep_data_test <- recipes::bake(object = prep_tte_recipe, new_data = data_test)

prep_data_train <- recipes::juice(prep_tte_recipe)


# inspect data ####
data_prep <- prep_data_train %>%
  bind_rows(prep_data_test)

#Uncomment next line to save the outcome
qs::qsave(data_prep, here::here("Data", "data_prep.qs"))


```



```{r}
data_prep  %>%
  skimr::skim()
```



# Setup model

## Construct the survival task

```{r mlr3}
library(mlr3learners)#extend mlr3 package withpopular learners, need it for using ranger
library(mlr3extralearners)#extended learners including surv.ranger
library(mlr3proba)#supports survival analysis
library(mlr3)#learners



# construction of Survival task ####

# First we put the data into an efficient memory data.table
# create instance
data_use <-
  mlr3::DataBackendDataTable$new(
    data = prep_data_train %>%
      dplyr::mutate(.id = as.integer(.id)) %>% data.table::as.data.table(),
    primary_key = ".id"
  )
# Specify the survival task, create new instance
surv_task <- mlr3proba::TaskSurv$new(
  id = "surv_example",
  backend = data_use,
  time = ".time",
  event = ".status",
  type = c("right")
)

```

Kaplan-Meier curve

```{r km_plot}
library(GGally)
# Explore Kaplan-Meier curve
mlr3viz::autoplot(surv_task)


```

## Build the learner (survival RF from ranger package)

```{r surv_learner}
library(ranger)
#built learner
ranger_lrn <- mlr3::lrn(
  "surv.ranger",
  respect.unordered.factors = "order",
  verbose = FALSE,
  importance = "permutation"
) #Variable importance mode
# Inspect parameters

ranger_lrn$param_set
```


# Hyperparameter tuning

Now it’s time to tune!

We will tune the following parameters for random forest:

- number_of_trees: number of trees in the forest
- mtry: number of variables to possibly split at in each node. 
- min_node_size: minimum number of observations in a terminal node


We will use mlr3 library for building a survival random forest and tuning the hyperparameters.

## Settings

### Set the parameter search space

```{r search_space}
library(paradox)

search_space <- paradox::ps(
  num.trees = paradox::p_int(lower = 500, upper = 2000),
  mtry = paradox::p_int(
    lower = floor(length(surv_task$col_roles$feature) * 0.1),
    upper = floor(length(surv_task$col_roles$feature) * 0.9)
  ),
  min.node.size = paradox::p_int(lower = 1, upper = 40)
)
search_space

```

### Select resampling strategy and performance measure

We need to specify how to evaluate the performance of a trained model. For this, 
we need to choose a resampling strategy and a performance measure. Here we choose cross-validation and C-index

```{r set_tune}

library(mlr3tuning)
#choose strategy and measure ####
#3-fold cross validation
hout <- mlr3::rsmp("cv", folds = 3)
measure <- mlr3::msr("surv.cindex")
```

### Generate tuning instance

```{r gen_tune}
#generate tuning instance, from task, learner, search space, resampling method and measure
instance <- mlr3tuning::TuningInstanceBatchSingleCrit$new(
  task = surv_task,
  learner = ranger_lrn,
  resampling = hout,
  measure = measure,
  search_space = search_space,
  terminator = mlr3tuning::trm("none")#evals5
)
```

## Define grid search

```{r surv_grid}
#The resolution is the number of distinct values to try per hyperparameter, 
#which means in our example the tuner will construct a 2x2x2 grid of 8 configurations 
#of equally spaced points between the specified upper and lower bounds
tuner <- mlr3tuning::tnr("grid_search", resolution = 2)
```

## Initiate tuning

- Note 1: actual_tuning chunk set to eval=FALSE
If one wants to run this chunk needs to set eval=TRUE. If one wants to save the 
outcome needs to uncoment the relevant line(see comments in the code)

- Note 2: We use parallelization, even though the dataset is small, for training purposes

```{r actual_tuning, eval = TRUE, message=FALSE, warning=FALSE}
#packages needed for parallelization
library(doFuture)
library(doRNG)
library(foreach)
tictoc::tic()
# enable parallel processing
doFuture::registerDoFuture()
future::plan(future::multisession, workers =  availableCores() - 1)

# specify seed
doRNG::registerDoRNG(seed = 123)


tuner$optimize(instance)
# disable parallel backend
foreach::registerDoSEQ()

tictoc::toc()

#Uncomment next line to save the outcome
#qs::qsave(instance, here::here("ML", "htune_demo.qs"))

```

## Evaluate tuning performance

How did all the possible parameter combinations do?

```{r evalhyper}
#instance <- qs::qread(here::here("ML", "htune_demo.qs"))
hyparams <- instance$search_space$ids()
perf_data <- instance$archive$data

perf_data %>%
  dplyr::select(num.trees, mtry,	min.node.size,	surv.cindex) %>%
  arrange(desc(surv.cindex)) %>%
  mutate(surv.cindex = surv.cindex %>% round(., digits = 4)) %>%
  kableExtra::kable(escape = FALSE) %>%
  kableExtra::kable_styling(
    bootstrap_options = "striped",
    full_width = FALSE,
    position = "left"
  ) %>%
  kableExtra::column_spec(1, bold = TRUE) %>%
  kableExtra::row_spec(0,
                       bold = TRUE,
                       background = "#666666",
                       color = "white")

```


# Final model training and performance evaluation

##  Train the final learner
Change hyperparameters to those selected in the tuning step

```{r add_best_hyperpar}
# adding best hyperparameters
ranger_lrn$param_set$values <- c(
  ranger_lrn$param_set$values,
  perf_data %>%
    select(num.trees, mtry,	min.node.size,	surv.cindex) %>%
    arrange(desc(surv.cindex)) %>%
    select(-surv.cindex) %>%
    slice(1)
)
```

Train the final learner

```{r final_learner}
set.seed(1234)
final_rf <- ranger_lrn$train(task = surv_task)
```


## Performance


Testing

```{r}
# predict the outcome with the test data
pred_test <- final_rf$predict_newdata(newdata = prep_data_test)
# Define the performance metrics
pred_measures <- suppressWarnings(mlr3::msrs("surv.cindex"))
# Estimate performance
test_performance <- pred_test$score(
  measures = pred_measures,
  task = surv_task,
  learner = final_rf,
  train_set = surv_task$row_ids
) %>%
  tibble::enframe(name = ".metric", value = ".estimate")
#print performance
test_performance %>%
  kableExtra::kable(escape = FALSE) %>%
  kableExtra::kable_styling(
    bootstrap_options = "striped",
    full_width = FALSE,
    position = "left"
  ) %>%
  kableExtra::column_spec(1, bold = TRUE) %>%
  kableExtra::row_spec(0,
                       bold = TRUE,
                       background = "#666666",
                       color = "white") 
```

Training

```{r}
# predict the outcome with the test data
pred_train <- final_rf$predict_newdata(newdata = prep_data_train)

# Estimate performance
train_performance <- pred_train$score(
  measures = pred_measures,
  task = surv_task,
  learner = final_rf,
  train_set = surv_task$row_ids
) %>%
  tibble::enframe(name = ".metric", value = ".estimate")
# Print performance
train_performance %>%
  kableExtra::kable(escape = FALSE) %>%
  kableExtra::kable_styling(
    bootstrap_options = "striped",
    full_width = FALSE,
    position = "left"
  ) %>%
  kableExtra::column_spec(1, bold = TRUE) %>%
  kableExtra::row_spec(0,
                       bold = TRUE,
                       background = "#666666",
                       color = "white") 
```


# Explainable AI

## Variable Feature Importance (VIMP)

Lastly, let’s learn about feature importance for this model.
For a ranger model, we do need to add in the engine importance = "permutation", in order to compute feature importance. 

```{r calc_vimp}
importance <- final_rf$importance() %>%
  as_tibble_col() %>%
  bind_cols(variables = final_rf$importance() %>% names()) %>%
  relocate(variables)

importance %>%
  DT::datatable(
    rownames = TRUE,
    filter = "top",
    selection = "single",
    extensions = c("Buttons"),
    options = list(
      lengthMenu = c(5, 10, 25, 50),
      pageLength = 5,
      scrollX = TRUE,
      dom = "lfrtBpi",
      buttons = list("excel")
    )
  )

```

```{r plot_top_vimp}
# top 10
vi_nplot <- 10

#plot permutation importance
imp_fr_plt <- importance %>%
  dplyr::arrange(., desc(value)) %>%
  dplyr::slice(1:vi_nplot) %>%
  dplyr::mutate(Sign = as.factor(ifelse(value > 0, "positive", "negative")))

p <- imp_fr_plt %>%
  ggplot(aes(
    y = reorder(variables, value),
    x = value,
    fill = Sign
  )) +
  geom_col() +
  scale_fill_manual(values = c("#00659C", "#930A34")) +
  theme(
    legend.position = "none",
    axis.title.y = element_blank(),
    plot.subtitle = element_text(size = 11),
    plot.title.position = "plot",
    plot.margin = margin(r = 20)
  ) +
  labs(subtitle = paste0("The top ", vi_nplot, " Variables based on Permutation"))


#plot output
p

```

## Partial dependency plots (PDPs) 

We will show partial dependency plots.
We will use all available data: train + test

```{r pdp_prep, eval=TRUE}
library(tidyverse)
library(ranger)
rf_model <- final_rf$model
# define time points represented by rank/order they appear
time_points <-
  seq(
    from = 1,
    to = length(rf_model$unique.death.times),
    length.out = 10
  ) %>% round()

```

Note: PDP predictions chunk set to eval=FALSE
If one wants to run this chunk needs to set eval=TRUE. If one wants to save the outcome needs to uncoment the relevant line(see comments in the code)

### bm02

```{r conti_pdps_predictions, eval=TRUE}


# define feature and grid (based onf feature class)
feat <- "bm02"

feat_cat <- data_prep %>% dplyr::pull(!!feat) %>% class()
n_grid <- 10


if (feat_cat == "numeric") {
  feat_range <- data_prep %>%
    dplyr::pull(!!feat) %>%
    range()
  
  feat_grid <-
    seq(from = feat_range[1],
        to = feat_range[2],
        length.out = n_grid)
  
} else {
  feat_grid <- data_prep %>%
    dplyr::pull(!!feat) %>%
    levels()
}

# replace corresponding feature values with grid values
data_sets <-  purrr::map(feat_grid,
                         ~ data_prep %>% dplyr::mutate(dplyr::across(
                           tidyselect::all_of(feat),
                           .fns = function(y)
                             .x
                         )))


# calculate predictions for specified time points ####
preds <- map(
  data_sets,
  ~ ranger:::predict.ranger(rf_model, data = .x)$survival %>%
    tibble::as_tibble() %>%
    dplyr::select(all_of(time_points))
)
# calculate PDPs (average (survival probability) for each feature grid value per timepoint)
pdp_data <- purrr::map2(
  preds,
  feat_grid,
  ~ .x %>%
    apply(2, mean) %>%
    tibble::enframe(value = ".value", name = "time_id") %>%
    dplyr::mutate(time_id = stringr::str_replace(time_id, "V", "") %>% as.numeric()) %>%
    dplyr::mutate(feat_val1 = .y)
) %>%
  dplyr::bind_rows() %>%
  dplyr::rename({
    {
      feat
    }
  } := feat_val1)

```

Plot PDPs

```{r conti_pdps_plot, eval = TRUE}
feat <- "bm02"
feat_cat <- data_prep %>% dplyr::pull(!!feat) %>% class()

#create rug for bm02
feat_rug <- data_prep %>%
  dplyr::pull(!!feat)
#plot
p <- pdp_data %>%
  ggplot(aes(x = !!rlang::sym(feat), y = 1 - .value)) + # 1 minus for event probability, not survival prob
  {
    if (feat_cat == "numeric")
      geom_line()
    else
      geom_col()
  } +
  facet_wrap(
    ~ time_id,
    nrow = 2,
    labeller =  ggplot2::labeller(
      time_id  = function(s) {
        rf_model$unique.death.times[as.numeric(s)] %>% round(3)
      },
      # construct time labels within the function
      .default = ggplot2::label_value
    )
  ) +
  ylab("event probability")

p + ggplot2::geom_rug(
  data = feat_rug %>%
    tibble::enframe(),
  mapping = ggplot2::aes(x = value),
  inherit.aes = F,
  sides = "b",
  alpha = 1,
  col = "#B3B3B3"
)
```


### .trt

Calculate PDPs

```{r trt_pdps_predictions, eval=TRUE}


# define feature and grid (based on feature class)
feat <- ".trt"

feat_cat <- data_prep %>% dplyr::pull(!!feat) %>% class()
n_grid <- 50


if (feat_cat == "numeric") {
  feat_range <- data_prep %>%
    dplyr::pull(!!feat) %>%
    range()
  
  feat_grid <-
    seq(from = feat_range[1],
        to = feat_range[2],
        length.out = n_grid)
  
} else {
  feat_grid <- data_prep %>%
    dplyr::pull(!!feat) %>%
    levels()
}

# replace corresponding feature values with grid values
data_sets <-  purrr::map(feat_grid,
                         ~ data_prep %>% dplyr::mutate(dplyr::across(
                           tidyselect::all_of(feat),
                           .fns = function(y)
                             .x
                         )))

# calculate predictions for specified time points
preds <- map(
  data_sets,
  ~ ranger:::predict.ranger(rf_model, data = .x)$survival %>%
    tibble::as_tibble() %>%
    dplyr::select(all_of(time_points))
)
# calculate PDPs (average (survival probability) for each feature grid value per timepoint)
pdp_data <- purrr::map2(
  preds,
  feat_grid,
  ~ .x %>%
    apply(2, mean) %>%
    tibble::enframe(value = ".value", name = "time_id") %>%
    dplyr::mutate(time_id = stringr::str_replace(time_id, "V", "") %>% as.numeric()) %>%
    dplyr::mutate(feat_val1 = .y)
) %>%
  dplyr::bind_rows() %>%
  dplyr::rename({
    {
      feat
    }
  } := feat_val1)

```

Plot PDPs

```{r trt_pdps_plot, eval=TRUE}

p<-pdp_data %>%
  ggplot(aes(x = !!rlang::sym(feat), y = 1 - .value)) + # 1 minus for event probability, not survival prob
  { if (feat_cat == "numeric") geom_line() else geom_col()} +
  facet_wrap(~ time_id, 
             nrow = 2, 
             labeller =  ggplot2::labeller(time_id  = function(s) {rf_model$unique.death.times[as.numeric(s)] %>% round(3) }, # construct time labels within the function
                                           .default = ggplot2::label_value)) +
  ylab("event probability")
p
```



# References

- [mlr3](https://mlr3.mlr-org.com/)
- [Tidymodels](https://www.tidymodels.org/)
- [Explanatory Model Analysis](https://ema.drwhy.ai/)


***


