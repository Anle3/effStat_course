---
output:
  html_document:
    highlight: tango
    number_sections: false
    theme: spacelab
    toc: true
    toc_depth: 4
    toc_float: true
    code_folding: hide
    includes:
      in_header: _style/_header.html
      before_body: _style/_before_body.html
      after_body: _style/_after_body.html
    css: _style/style.css
    self_contained: true

# program specification

params:
  project:
    label: "Project"
    value: ""
    input: text
  study:
    label: "Study"
    value: ""
    input: text
  version:
    label: "Program version"
    value: "0.1"
    input: text
  programmer:
    label: "Programmer"
    value: "[Eliana Garcia Cossio](mailto:eliana.garciacossio@bayer.com) (GLRXV),[Antigoni Elefsinioti](mailto:Antigoni.Elefsinioti@bayer.com) (GECOO)"
    input: text
  input:
    label: "Input"
    value: "see [Data import](#import)"
    input: text
  clin_data:
    label: "Clinical data origin (if applicable)"
    value: ""
    input: text
  steps:
    label: "Analysis stpes"
    value: "see TOC"
    input: text
  output:
    label: "Output"
    value: ""
    input: text
  validation_level:
    label: "Validation level"
    value: 0
    input: select
    choices: [0, 1, 2, 3]
  validation_status:
    label: "Validation status"
    value: "preliminary (not validated)"
    input: select
    choices: [preliminary (not validated), validated]

title: "SDI AI/ML training"
subtitle: "Demo RSF on simulated data"
---
This demo aims at providing an overview of applying Random Survival Forests on simulated data.
We are splitting the demo in the following steps

1. Data preprocessing
2. Hyperparameter tuning
3. Performace evaluation
4. VIMP (Variable Importance)
5. PDPs (Partial Dependency Plots)

Dataset simulation:


We simulate data and also right-censored survival outcome, using a proportional hazard model with time-constant baseline hazard, such as 

$Surv(time,status) \sim HCT+BPSYS+trt+trt:BMI$

Where:

* Main effects
+ trt:Treatment 
+ HCT: Hematocrit
+ BPSYS:Systolic blood pressure 
* Interaction with treatment
+ BMI:  Body mass index

The dataset is small on purpose, so the knitting runtime is short. Still some chunks might take a bit longer to run . We set these chunks to eval=FALSE and saved their outcomes  in .qs files. Those files are being read in the next chunk. If one wants to run these chunks please set eval to TRUE

```{r knitsetup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, out.width = "100%")
```

```{r import_packages}
# load library TV
library(tidyverse)
#load utility packages
library(kableExtra)
library(here)
library(tictoc)
library(stringr)
library(qs)
library(skimr)
library(ggplot2)
```

```{r options, include=FALSE}


ggplot2::theme_set(theme_light() +
            theme(
              strip.background = element_rect(fill = "#00617F"),
              strip.text = element_text(colour = "#FFFFFF")
            )
)

```





<!-- start of analysis ---------------------------------------------------- -->
# Data import
```{r reading_data}


data<-qs::qread(here::here("Data","Demo_data_tte.qs"))

```


# Exploring the data

```{r raw_data_explore, skimr_digits = 2}


data%>% skimr::skim()
 
```


# Data initial split

We can start by loading the tidymodels metapackage and splitting our data into training and testing sets 

```{r split_data}
library(tidymodels)

set.seed(123)
#create a single binary split of the data into a training set and testing set
data_split <- rsample::initial_split(data, strata = .status)
#extract  the resulting data 
data_train <- rsample::training(data_split)
data_test <- rsample::testing(data_split)


```



# Pre-processing data

We pre-process the training data and apply exactly the same step to the test data.

- Imputing the missing data using k-nearest neighboors
- Normalization
- Near zero variance features
- Correlation filter with r= 0.9



```{r data_preprocess}
#textrecipes contain extra steps for the recipes package for preprocessing text data.
library(textrecipes)
# make a recipe ####
tte_recipe <-
  recipes::recipe(formula = .time + .status ~ ., data = data_train) %>%
  recipes::update_role(.id, new_role = "id") %>%
  recipes::update_role(c(.time, .status), new_role = "outcome") %>%
  recipes::step_impute_knn(recipes::all_predictors(), -.trt) %>%
  recipes::step_naomit(recipes::all_predictors()) %>%
  recipes::step_nzv(recipes::all_predictors(),
                    freq_cut = 95 / 5,
                    unique_cut = 10) %>%
  recipes::step_normalize(recipes::all_numeric_predictors()) %>%
  recipes::step_corr(recipes::all_numeric_predictors(), threshold = 0.9)#%>%


#prepare new data####
prep_tte_recipe <- tte_recipe %>%
  recipes::prep()

prep_data_test <-
  recipes::bake(object = prep_tte_recipe, new_data = data_test)
prep_data_train <- recipes::juice(prep_tte_recipe)


# inspect data ####
data_prep <- prep_data_train %>%
  bind_rows(prep_data_test)
data_prep  %>%
  skimr::skim()

```


# Setup model

## Construct the survival task

```{r mlr3}
library(mlr3learners)#extend mlr3 package withpopular learners, need it for using ranger
library(mlr3proba)#supports survival analysis
library(mlr3)#learners



# construction of Survival task ####

# First we put the data into an efficient memory data.table
# create instance
data_use <-
  mlr3::DataBackendDataTable$new(
    data = prep_data_train %>%
      dplyr::mutate(.id = as.integer(.id)) %>% data.table::as.data.table(),
    primary_key = ".id"
  )
# Specify the survival task, create new instance
surv_task <- mlr3proba::TaskSurv$new(
  id = "surv_example",
  backend = data_use,
  time = ".time",
  event = ".status",
  type = c("right")
)

```

Kaplan-Meier curve

```{r km_plot}
# Explore Kaplan-Meier curve
mlr3viz::autoplot(surv_task)


```

## Build the learner (survival RF from ranger package)

```{r surv_learner}
#built learner
ranger_lrn <- mlr3::lrn(
  "surv.ranger",
  respect.unordered.factors = "order",
  verbose = FALSE,
  importance = "permutation"
) #Variable importance mode
# Inspect parameters

ranger_lrn$param_set
```


# Tuning


Now itâ€™s time to tune!

We will tune the following parameters for random forest:

- number_of_trees
- mtry, number of variables to possibly split at in each node. 
- min_node_size


We will use mlr3 library for building a survival random forest and tunning the hyperparameters.

## Settings

### Set the parameter search space

```{r search_space}
library(paradox)
search_space <- paradox::ps(
  num.trees = paradox::p_int(lower = 500, upper = 2000),
  mtry = paradox::p_int(
    lower = floor(length(surv_task$col_roles$feature) * 0.1),
    upper = floor(length(surv_task$col_roles$feature) * 0.9)
  ),
  min.node.size = paradox::p_int(lower = 1, upper = 40)
)
search_space

```

### Select resampling strategy and performance measure

We need to specify how to evaluate the performance of a trained model. For this, we need to choose a resampling strategy and a performance measure. Here we choose cross-validation and C-index

```{r set_tune}

library(mlr3tuning)
#choose strategy and measure ####
#3-fold cross validation
hout <- mlr3::rsmp("cv", folds = 3)
measure <- mlr3::msr("surv.cindex")
#Terminator that stops after a number of evaluations
evals5 = mlr3tuning::trm("evals", n_evals = 5)
```

### Generate tuning instance

```{r gen_tune}
#generate tuning instance, from task, learner, search space, resampling method and measure
instance <- mlr3tuning::TuningInstanceSingleCrit$new(
  task = surv_task,
  learner = ranger_lrn,
  resampling = hout,
  measure = measure,
  search_space = search_space,
  terminator = evals5
)
```

## Define grid search

```{r surv_grid}
tuner <- mlr3tuning::tnr("grid_search", resolution = 5)
```

## Initiate tuning

Note 1: actual_tuning chunk set to eval=FALSE
If one wants to run this chunk needs to set eval=TRUE. If one wants to save the outcome needs to uncoment the relevant line(see comments in the code)

Note 2: We use  parallelization, even though dataset small, for training purposes
```{r actual_tuning, eval = FALSE}
#packages needed for parallelization
library(doFuture)
library(doRNG)
library(foreach)
tictoc::tic()
# enable parallel processing
doFuture::registerDoFuture()
future::plan(future::multisession, workers =  availableCores() - 1)

# specify seed
doRNG::registerDoRNG(seed = 123)


tuner$optimize(instance)
# disable parallel backend
foreach::registerDoSEQ()

tictoc::toc()

#Uncomment next line to save the outcome
qs::qsave(instance, here::here("Data", "htune_demo.qs"))

```

## Evaluate tuning performance

How did all the possible parameter combinations do?

```{r eval_hyper}
instance <- qs::qread(here::here("Data", "htune_demo.qs"))
hyparams <- instance$search_space$ids()
perf_data <- instance$archive$data

perf_data %>%
  select(num.trees, mtry,	min.node.size,	surv.harrell_c) %>%
  arrange(desc(surv.harrell_c)) %>%
  mutate(surv.harrell_c = surv.harrell_c %>% round(., digits = 4)) %>%
  kableExtra::kable(escape = FALSE) %>%
  kableExtra::kable_styling(
    bootstrap_options = "striped",
    full_width = FALSE,
    position = "left"
  ) %>%
  kableExtra::column_spec(1, bold = TRUE) %>%
  kableExtra::row_spec(0,
                       bold = TRUE,
                       background = "#00617F",
                       color = "white")

```

# Final model

##  Train the final learner
Change hyperparameters to those selected in the tuning step

```{r add_best_hyperpar}
# adding best hyperparameters
ranger_lrn$param_set$values <- c(
  ranger_lrn$param_set$values,
  perf_data %>%
    select(num.trees, mtry,	min.node.size,	surv.harrell_c) %>%
    arrange(desc(surv.harrell_c)) %>%
    select(-surv.harrell_c) %>%
    slice(1)
)
```

Train the final learner

```{r final_learner}
set.seed(1234)
final_rf <- ranger_lrn$train(task = surv_task)
```


## Performance


Testing

```{r}
# predict the outcome with the test data
pred_test <- final_rf$predict_newdata(newdata = prep_data_test)
# Define the performance metrics
pred_measures <- suppressWarnings(mlr3::msrs("surv.cindex"))
# Estimate performance
test_performance <- pred_test$score(
  measures = pred_measures,
  task = surv_task,
  learner = final_rf,
  train_set = surv_task$row_ids
) %>%
  tibble::enframe(name = ".metric", value = ".estimate")
#print performance
test_performance %>%
  kableExtra::kable(escape = FALSE) %>%
  kableExtra::kable_styling(
    bootstrap_options = "striped",
    full_width = FALSE,
    position = "left"
  ) %>%
  kableExtra::column_spec(1, bold = TRUE) %>%
  kableExtra::row_spec(0,
                       bold = TRUE,
                       background = "#00617F",
                       color = "white") 
```

Training

```{r}
# predict the outcome with the test data
pred_train <- final_rf$predict_newdata(newdata = prep_data_train)

# Estimate performance
train_performance <- pred_train$score(
  measures = pred_measures,
  task = surv_task,
  learner = final_rf,
  train_set = surv_task$row_ids
) %>%
  tibble::enframe(name = ".metric", value = ".estimate")
# Print performance
train_performance %>%
  kableExtra::kable(escape = FALSE) %>%
  kableExtra::kable_styling(
    bootstrap_options = "striped",
    full_width = FALSE,
    position = "left"
  ) %>%
  kableExtra::column_spec(1, bold = TRUE) %>%
  kableExtra::row_spec(0,
                       bold = TRUE,
                       background = "#00617F",
                       color = "white") 
```

# Variable Feature Importance (VIMP)

Lastly, letâ€™s learn about feature importance for this model using the vip package. For a ranger model, we do need to add in the engine importance = "permutation", in order to compute feature importance. 

```{r calc_vimp}
importance <- final_rf$importance() %>%
  as_tibble_col() %>%
  bind_cols(variables = final_rf$importance() %>% names()) %>%
  relocate(variables)

importance %>%
  DT::datatable(
    rownames = TRUE,
    filter = "top",
    selection = "single",
    extensions = c("Buttons"),
    options = list(
      lengthMenu = c(5, 10, 25, 50),
      pageLength = 5,
      scrollX = TRUE,
      dom = "lfrtBpi",
      buttons = list("excel")
    )
  )

```

```{r plot_top_vimp}
# top 10
vi_nplot <- 10

#plot permutation importance
imp_fr_plt <- importance %>%
  dplyr::arrange(., desc(value)) %>%
  dplyr::slice(1:vi_nplot) %>%
  dplyr::mutate(Sign = as.factor(ifelse(value > 0, "positive", "negative")))

p <- imp_fr_plt %>%
  ggplot(aes(
    y = reorder(variables, value),
    x = value,
    fill = Sign
  )) +
  geom_col() +
  scale_fill_manual(values = c("#00659C", "#930A34")) +
  theme(
    legend.position = "none",
    axis.title.y = element_blank(),
    plot.subtitle = element_text(size = 11),
    plot.title.position = "plot",
    plot.margin = margin(r = 20)
  ) +
  labs(subtitle = paste0("The top ", vi_nplot, " Variables based on Permutation"))


#plot output
p

```


# Explainable AI (PDPs)

We will show partial dependency plots.
We will use all available data: train + test

## HCT

```{r pdp_prep}
library(tidyverse)
library(ranger)
rf_model <- final_rf$model
# define time points represented by rank/order they appear
time_points <-
  seq(
    from = 1,
    to = length(rf_model$unique.death.times),
    length.out = 10
  ) %>% round()

```
Note: PDP predictions chunk set to eval=FALSE
If one wants to run this chunk needs to set eval=TRUE. If one wants to save the outcome needs to uncoment the relevant line(see comments in the code)
```{r hct_pdps_predictions, eval=FALSE}


# define feature and grid (based onf feature class)
feat <- "HCT"

feat_cat <- data_prep %>% dplyr::pull(!!feat) %>% class()
n_grid <- 50


if (feat_cat == "numeric") {
  feat_range <- data_prep %>%
    dplyr::pull(!!feat) %>%
    range()
  
  feat_grid <-
    seq(from = feat_range[1],
        to = feat_range[2],
        length.out = n_grid)
  
} else {
  feat_grid <- data_prep %>%
    dplyr::pull(!!feat) %>%
    levels()
}

# replace corresponding feature values with grid values
data_sets <-  purrr::map(feat_grid,
                         ~ data_prep %>% dplyr::mutate(dplyr::across(
                           tidyselect::all_of(feat),
                           .fns = function(y)
                             .x
                         )))


# calculate predictions for specified time points ####
preds <- map(
  data_sets,
  ~ ranger:::predict.ranger(rf_model, data = .x)$survival %>%
    tibble::as_tibble() %>%
    dplyr::select(all_of(time_points))
)
# calculate PDPs (average (survival probability) for each feature grid value per timepoint)
pdp_data <- purrr::map2(
  preds,
  feat_grid,
  ~ .x %>%
    apply(2, mean) %>%
    tibble::enframe(value = ".value", name = "time_id") %>%
    dplyr::mutate(time_id = stringr::str_replace(time_id, "V", "") %>% as.numeric()) %>%
    dplyr::mutate(feat_val1 = .y)
) %>%
  dplyr::bind_rows() %>%
  dplyr::rename({
    {
      feat
    }
  } := feat_val1)

#Uncomment next line to save the outcome
qs::qsave(pdp_data, here::here("Data", "hct_pdp_preds.qs"))
```

Plot PDPs

```{r hct_pdps_plot}
feat <- "HCT"
pdp_data <- qs::qread(here::here("Data", "hct_pdp_preds.qs"))
feat_cat <- data_prep %>% dplyr::pull(!!feat) %>% class()

#create rug for HCT
hct_rug <- data_prep %>%
  dplyr::pull(!!feat)
#plot
p <- pdp_data %>%
  ggplot(aes(x = !!rlang::sym(feat), y = 1 - .value)) + # 1 minus for event probability, not survival prob
  {
    if (feat_cat == "numeric")
      geom_line()
    else
      geom_col()
  } +
  facet_wrap(
    ~ time_id,
    nrow = 2,
    labeller =  ggplot2::labeller(
      time_id  = function(s) {
        rf_model$unique.death.times[as.numeric(s)] %>% round(3)
      },
      # construct time labels within the function
      .default = ggplot2::label_value
    )
  ) +
  ylab("event probability")

p + ggplot2::geom_rug(
  data = hct_rug %>%
    tibble::enframe(),
  mapping = ggplot2::aes(x = value),
  inherit.aes = F,
  sides = "b",
  alpha = 1,
  col = "#B3B3B3"
)
```


## .trt
Calculate PDPs
```{r trt_pdps_predictions}


# define feature and grid (based on feature class)
feat <- ".trt"

feat_cat <- data_prep %>% dplyr::pull(!!feat) %>% class()
n_grid <- 50


if (feat_cat == "numeric") {
  feat_range <- data_prep %>%
    dplyr::pull(!!feat) %>%
    range()
  
  feat_grid <-
    seq(from = feat_range[1],
        to = feat_range[2],
        length.out = n_grid)
  
} else {
  feat_grid <- data_prep %>%
    dplyr::pull(!!feat) %>%
    levels()
}

# replace corresponding feature values with grid values
data_sets <-  purrr::map(feat_grid,
                         ~ data_prep %>% dplyr::mutate(dplyr::across(
                           tidyselect::all_of(feat),
                           .fns = function(y)
                             .x
                         )))

# calculate predictions for specified time points
preds <- map(
  data_sets,
  ~ ranger:::predict.ranger(rf_model, data = .x)$survival %>%
    tibble::as_tibble() %>%
    dplyr::select(all_of(time_points))
)
# calculate PDPs (average (survival probability) for each feature grid value per timepoint)
pdp_data <- purrr::map2(
  preds,
  feat_grid,
  ~ .x %>%
    apply(2, mean) %>%
    tibble::enframe(value = ".value", name = "time_id") %>%
    dplyr::mutate(time_id = stringr::str_replace(time_id, "V", "") %>% as.numeric()) %>%
    dplyr::mutate(feat_val1 = .y)
) %>%
  dplyr::bind_rows() %>%
  dplyr::rename({
    {
      feat
    }
  } := feat_val1)

```

Plot PDPs

```{r trt_pdps_plot}

p<-pdp_data %>%
  ggplot(aes(x = !!rlang::sym(feat), y = 1 - .value)) + # 1 minus for event probability, not survival prob
  { if (feat_cat == "numeric") geom_line() else geom_col()} +
  facet_wrap(~ time_id, 
             nrow = 2, 
             labeller =  ggplot2::labeller(time_id  = function(s) {rf_model$unique.death.times[as.numeric(s)] %>% round(3) }, # construct time labels within the function
                                           .default = ggplot2::label_value)) +
  ylab("event probability")
p
```

## .trt and BMI,2-d pdp
Calculate 2D PDPs for BMI,.trt
Note: PDP predictions chunk set to eval=FALSE
If one wants to run this chunk needs to set eval=TRUE. If one wants to save the outcome needs to uncoment the relevant line(see comments in the code)

```{r interaction_pdps_predictions, eval=FALSE}


# define grid ####
feat <- c("BMI", ".trt")

n_grid <- 50

#create range for BMI
bmi_range <- data_prep %>%
  dplyr::pull(BMI) %>%
  range()

bmi_grid <-
  seq(from = bmi_range[1],
      to = bmi_range[2],
      length.out = n_grid)

#get trt levels
trt_grid <- data_prep %>%
  dplyr::pull(.trt) %>%
  levels()


# replace corresponding feature values with grid values
data_sets <-
  tidyr::expand_grid(BMI = bmi_grid, .trt = trt_grid, data_prep %>% select(-c(BMI, .trt)))

# calculate predictions for specified time points

preds <-
  ranger:::predict.ranger(rf_model, data = data_sets)$survival %>%
  tibble::as_tibble() %>%
  dplyr::select(all_of(time_points))

#merge predictions with data set and calculate 2D PDPs (average (survival probability) for each feature combination value per timepoint)
pdp_data  <- data_sets %>% bind_cols(preds) %>%
  filter(BMI %in% bmi_grid) %>%
  pivot_longer(
    c(colnames(preds)),
    names_to = "time_id",
    values_to = ".values"
  )  %>% mutate(time_id = stringr::str_replace(time_id, "V", "") %>% as.numeric()) %>%
  group_by(BMI, .trt, time_id) %>% summarise(.value = mean(.values))
#Uncomment next line to save the outcome
qs::qsave(pdp_data, here::here("Data", "bmi_trt_int_pdp_preds.qs"))
```

Plot PDPs
```{r interaction_pdps_plot}
#rug
bmi_rug <- data_prep %>%
  dplyr::pull(BMI)

pdp_data <- qs::qread(here::here("Data", "bmi_trt_int_pdp_preds.qs"))
#create range for BMI
bmi_range <- data_prep %>%
  dplyr::pull(BMI) %>%
  range()
#plot
p <- pdp_data %>%
  ggplot2::ggplot(aes(
    x = BMI,
    y = 1 - .value,
    color = .trt
  )) + # 1 minus for event probability, not survival prob
  geom_line()    +
  facet_wrap(
    ~ time_id,
    nrow = 2,
    labeller =  ggplot2::labeller(
      time_id  = function(s) {
        rf_model$unique.death.times[as.numeric(s)] %>% round(3)
      },
      # construct time labels within the function
      .default = ggplot2::label_value
    )
  ) +
  ylab("event probability")

p + ggplot2::geom_rug(
  data = bmi_rug %>% tibble::enframe(),
  mapping = ggplot2::aes(x = value),
  inherit.aes = F,
  sides = "b",
  alpha = 1,
  col = "#B3B3B3"
) +
  coord_cartesian(ylim = c(0, 0.5))
```



# References

- [mlr3](https://mlr3.mlr-org.com/)
- [Tidymodels](https://www.tidymodels.org/)
- [Explanatory Model Analysis](https://ema.drwhy.ai/)


***

# {.toc-ignore #sessioninfo}

Session info
```{r session, echo=FALSE, comment=""}
Hmisc::markupSpecs$html$session()
```

Program: ``r knitr::current_input(dir = TRUE)``